{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyOZvdORjLb4JNS0kxa11kia"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"qMjEwHH-fV_l"},"outputs":[],"source":["!pip install chromadb\n","!pip install bitsandbytes\n","!pip install accelerate\n","!pip install transformers\n","!pip install langchain\n","!pip install unstructured\n","!pip install sentence_transformers"]},{"cell_type":"code","source":["import torch\n","from transformers import BitsAndBytesConfig, AutoTokenizer, AutoModelForCausalLM\n","\n","def load_model(model_id: str) -> dict:\n","    try:\n","        # Define configuration for BitsAndBytes model\n","        nf4_config = BitsAndBytesConfig(\n","            load_in_4bit=True,\n","            bnb_4bit_quant_type=\"nf4\",\n","            bnb_4bit_use_double_quant=True,\n","            bnb_4bit_compute_dtype=torch.bfloat16\n","        )\n","\n","        # Load tokenizer and model\n","        model = AutoModelForCausalLM.from_pretrained(\n","            model_id,\n","            torch_dtype=torch.bfloat16,\n","            device_map=\"auto\",\n","            quantization_config=nf4_config,\n","            token=\"Your_HF_Token\",\n","            low_cpu_mem_usage=True\n","        )\n","        tokenizer = AutoTokenizer.from_pretrained(model_id, token=\"Your_HF_Token\")\n","\n","        return {\"model\": model, \"tokenizer\": tokenizer}\n","    except Exception as e:\n","        print(f\"Error loading model: {e}\")\n","        return None\n","\n"],"metadata":{"id":"upnUyyHcgS9H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from langchain.embeddings import HuggingFaceEmbeddings\n","\n","def load_huggingface_embeddings(model_name: str, model_kwargs: dict) -> HuggingFaceEmbeddings:\n","    \"\"\"\n","    Loads Hugging Face embeddings model.\n","\n","    Args:\n","        model_name (str): Name of the Hugging Face model.\n","        model_kwargs (dict): Keyword arguments to pass to the Hugging Face model.\n","\n","    Returns:\n","        HuggingFaceEmbeddings: Hugging Face embeddings model.\n","    \"\"\"\n","    try:\n","        # Instantiate HuggingFaceEmbeddings\n","        embeddings = HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs)\n","        return embeddings\n","    except Exception as e:\n","        print(f\"Error loading Hugging Face embeddings model: {e}\")\n","        return None"],"metadata":{"id":"IxaC5OFDgS6l"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"i7KpQ5ocgS4V"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from langchain.document_loaders import DirectoryLoader\n","from langchain.text_splitter import RecursiveCharacterTextSplitter\n","from langchain.schema import Document\n","from langchain.vectorstores.chroma import Chroma\n","import os\n","from langchain_community.document_loaders import PyPDFLoader\n","\n","\n","\n","def process_documents_and_query(DATA_PATH: str, embeddings, CHROMA_PATH: str) -> str:\n","    \"\"\"\n","    Process documents from a given directory, perform a similarity search based on the query text,\n","    and return the concatenated context text if matching results are found.\n","\n","    Args:\n","        DATA_PATH (str): Path to the directory containing text documents.\n","        CHROMA_PATH (str): Path to the directory where the Chroma database will be stored.\n","        query_text (str): Text to be used for similarity search.\n","        k (int, optional): Number of top results to retrieve. Defaults to 3.\n","\n","    Returns:\n","        str: Concatenated context text if matching results are found, otherwise an empty string.\n","    \"\"\"\n","    loader = DirectoryLoader(DATA_PATH, glob=\"*.txt\")\n","    documents = loader.load()\n","\n","    text_splitter = RecursiveCharacterTextSplitter(\n","        chunk_size=300,\n","        chunk_overlap=100,\n","       length_function=len,\n","        add_start_index=True,\n","    )\n","    chunks = text_splitter.split_documents(documents)\n","    print(f\"Split {len(documents)} documents into {len(chunks)} chunks.\")\n","    document = chunks[0]\n","    print(document.page_content)\n","    print(document.metadata)\n","    db = Chroma.from_documents(documents=chunks, embedding=embeddings, persist_directory=\"chroma_db\")\n","\n","    return db\n","\n","\n","def find_similarity(db, query_text, k: int = 3):\n","  results = db.similarity_search_with_relevance_scores(query_text, k=k)\n","  if len(results) == 0 or results[0][1] < 0.7:\n","        print(f\"Unable to find matching results.\")\n","        context_text = \"\"\n","  else:\n","        context_text = \"\\n\\n---\\n\\n\".join([doc.page_content for doc, _score in results])\n","\n","  return context_text\n","\n"],"metadata":{"id":"NinS_AGugS11"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from langchain.prompts import ChatPromptTemplate\n","\n","def generate_response_with_context(context_text: str, query_text: str, model, tokenizer):\n","    \"\"\"\n","    Generate a response to a query based on the provided context using a language model.\n","\n","    Args:\n","        context_text (str): The context text used to generate the response.\n","        query_text (str): The query text for which a response is generated.\n","        model: The language model used for response generation.\n","        tokenizer: The tokenizer associated with the language model.\n","\n","    Returns:\n","        str: The generated response.\n","    \"\"\"\n","    PROMPT_TEMPLATE = \"\"\"\n","    \"You are youth from Dominican Republic.\"\n","    Answer the question based only on the following context:\n","    If the context is blank string, then just use your own knowledge\n","\n","    {context}\n","\n","    ---\n","    \"\"\"\n","\n","    prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)\n","    prompt = prompt_template.format(context=context_text)\n","    print(prompt)\n","\n","    messages = [\n","        {\"role\": \"system\", \"content\": prompt},\n","        {\"role\": \"user\", \"content\": query_text},\n","    ]\n","\n","    input_ids = tokenizer.apply_chat_template(\n","        messages,\n","        add_generation_prompt=True,\n","        return_tensors=\"pt\"\n","    ).to(model.device)\n","\n","\n","    terminators = [\n","        tokenizer.eos_token_id,\n","        tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n","    ]\n","\n","    outputs = model.generate(\n","        input_ids,\n","        max_new_tokens=256,\n","        eos_token_id=terminators,\n","        do_sample=True,\n","        temperature=0.6,\n","        top_p=0.9,\n","    )\n","    response = outputs[0][input_ids.shape[-1]:]\n","\n","    return tokenizer.decode(response, skip_special_tokens=True)\n","\n","\n"],"metadata":{"id":"SlfyItlRgSzw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n","loaded_model = load_model(model_id)\n","if loaded_model is None:\n","    print(\"Failed to load model.\")\n"],"metadata":{"id":"aZoJar-BgSwl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install chromadb"],"metadata":{"id":"6VZyV62HgSul"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import time\n","start_time = time.time()\n","import numpy as np\n","model = loaded_model[\"model\"]\n","tokenizer = loaded_model[\"tokenizer\"]\n","print(\"Model loaded successfully!\")\n","\n","model_name = \"sentence-transformers/all-mpnet-base-v2\"\n","model_kwargs = {\"device\": \"cuda\"}\n","embeddings = load_huggingface_embeddings(model_name, model_kwargs)"],"metadata":{"id":"pmHVIAKDgSq1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["DATA_PATH = \"/content/data\"\n","CHROMA_PATH = \"chroma\"\n","db = process_documents_and_query(DATA_PATH, embeddings, CHROMA_PATH)"],"metadata":{"id":"hGMEjLE-gSol"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["query_text = \"Who is the current president of the Dominican Republic?\"\n","context_text = find_similarity(db, query_text)\n","#print(\"Context Text:\", context_text)\n","response = generate_response_with_context(context_text, query_text, model, tokenizer)\n"],"metadata":{"id":"arpTZyjSgSmM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"Question: \", query_text)\n","print(\"Answer: \", response)\n","end_time = time.time()"],"metadata":{"id":"BjjOTi5zgSjN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###The End\n"],"metadata":{"id":"uL2yMtEPg0Jd"}},{"cell_type":"code","source":[],"metadata":{"id":"AHls4fdrgq3s"},"execution_count":null,"outputs":[]}]}